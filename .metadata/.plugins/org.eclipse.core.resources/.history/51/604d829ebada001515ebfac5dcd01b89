'''
Created on Feb 23, 2016

@author: rpulekar
'''
from pyspark import SparkConf, SparkContext
import string
import sys
# Set the Spark configuration so WordCount is run locally
conf = SparkConf().setMaster('local').setAppName('Assignment4_Problem4')
sc = SparkContext(conf = conf)
# Read all-bible.txt into RDD (Variable called RDDvar)
input_file = sys.argv[1]
output_dir = sys.argv[2]
RDDvar = sc.textFile(input_file)
# Tokenize each line
words = RDDvar.flatMap(lambda line: line.split())
# Converts tokens to lower-case & removes punctuation before creating a tuple (token, 1)
result = words.map(lambda word: (str(word.lower()).translate(None,string.punctuation), 1))
aggreg1 = result.reduceByKey(lambda a, b: a+b).sortByKey()
aggreg1.saveAsTextFile(output_dir)