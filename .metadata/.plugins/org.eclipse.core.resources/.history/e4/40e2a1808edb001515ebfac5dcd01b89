'''
Created on Feb 23, 2016

@author: rpulekar
'''
from pyspark import SparkConf, SparkContext
import string
import sys

# Set the Spark configuration
conf = SparkConf().setAppName('Assignment4_Problem3')

# Set the spark context
sparkContext = SparkContext(conf = conf)

# set input/output filenames/dirs
input_file_name = sys.argv[1]
output_dir_name = sys.argv[2]

inputFileContents = sparkContext.textFile(input_file)

# Tokenize each line
words = inputFileContents.flatMap(lambda line: line.split())
# Converts tokens to lower-case & removes punctuation before creating a tuple (token, 1)
result = words.map(lambda word: (str(word.lower()).translate(None,string.punctuation), 1))
aggreg1 = result.filter(lambda keyValue: len(keyValue[0])>0)
aggreg1 = aggreg1.reduceByKey(lambda a, b: a+b).sortByKey()
aggreg1.saveAsTextFile(output_dir)